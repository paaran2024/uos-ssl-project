@inproceedings{liu2025catanet,
  title={{CATANet: Efficient Content-Aware Token Aggregation for Lightweight Image Super-Resolution}},
  author={Xin Liu and Jie Liu and Jie Tang and Gangshan Wu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}

@article{joshi2024importance,
    title={On Importance of Pruning and Distillation for Efficient Low Resource NLP}, 
    author={Aishwarya Mirashi and Purva Lingayat and Srushti Sonavane and Tejas Padhiyar and Raviraj Joshi and Geetanjali Kale},
    year={2024},
    eprint={2409.14162},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2409.14162}, 
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={6840--6851},
  year={2020}
}

@INPROCEEDINGS{he2020fakd,
  author={He, Zibin and Dai, Tao and Lu, Jian and Jiang, Yong and Xia, Shu-Tao},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={Fakd: Feature-Affinity Based Knowledge Distillation for Efficient Image Super-Resolution}, 
  year={2020},
  volume={},
  number={},
  pages={518-522},
  keywords={Knowledge engineering;Computational modeling;Correlation;Image resolution;Task analysis;Feature extraction;Mathematical model;Image super-resolution;Knowledge distillation;Model compression;Convolutional neural networks},
  doi={10.1109/ICIP40778.2020.9190917}}

@inproceedings{khaki2024optin,
    title={The Need for Speed: Pruning Transformers with One Recipe},
    author={Samir Khaki and Konstantinos N Plataniotis},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=MVmT6uQ3cQ}
}