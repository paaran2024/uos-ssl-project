\section{Discussion}

\subsection{Effectiveness of Automated Pruning}
Our results demonstrate that OPTIN-based automated pruning can effectively identify redundant components in CATANet's hybrid CNN-Transformer architecture. With 50\% MAC constraint, we achieve 51.95\% FLOPs reduction while maintaining the structural integrity needed for subsequent KD recovery. This suggests that manually designed architectures like CATANet contain significant redundancy that can be automatically removed.

\subsection{KD Strategy Comparison}
Output KD shows the best performance (38.15 dB) with simpler implementation, while Feature KD (38.09 dB) provides comparable results with richer intermediate supervision. FaKD is expected to outperform both by capturing spatial affinity relationships, though it requires higher computational cost during training due to affinity matrix computation.

\subsection{Limitations}
Our approach has several limitations: (1) The pruned model weights are currently masked rather than physically removed, requiring model rebuilding for actual parameter reduction. (2) Flutter's TFLite runtime does not natively support all Transformer operations, requiring additional conversion steps for mobile deployment. (3) FaKD training encounters CUDA memory constraints due to affinity matrix computation, limiting batch size.

\subsection{Future Work}
Future directions include: physical model rebuilding for actual parameter reduction, exploring hybrid KD strategies combining output and affinity-based supervision, and optimizing the model conversion pipeline for efficient mobile inference.
