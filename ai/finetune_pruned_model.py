import argparse
import torch
import torch.optim as optim
import torch.nn.functional as F # FaKD êµ¬í˜„ì„ ìœ„í•´ ì¶”ê°€
from torch.utils.data import DataLoader
import yaml
from tqdm import tqdm
import os
import csv
import pandas as pd
import matplotlib.pyplot as plt

# --- ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from scripts.load_catanet import get_catanet_teacher_model
from basicsr.data import build_dataset
from basicsr.metrics import calculate_psnr
from basicsr.utils import tensor2img
from utils.catanet_hooks import CATANetModelHooking

# --- ê²°ê³¼ ì €ì¥ ê²½ë¡œ ---
RESULTS_DIR = "results"
LOGS_DIR = os.path.join(RESULTS_DIR, "logs")
PLOTS_DIR = os.path.join(RESULTS_DIR, "plots")
SUMMARY_DIR = os.path.join(RESULTS_DIR, "summary")

"""
finetune_pruned_model.py: ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)ë¥¼ ì‚¬ìš©í•˜ì—¬
                           ê°€ì§€ì¹˜ê¸°ëœ(pruned) ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ì œ ì„¸ ê°€ì§€ ì¦ë¥˜ ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:
1.  Output Distillation: êµì‚¬ ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ì„ í•™ìƒ ëª¨ë¸ì´ ëª¨ë°©í•©ë‹ˆë‹¤. (ê¸°ë³¸)
2.  Feature Distillation: êµì‚¬ ëª¨ë¸ì˜ ì¤‘ê°„ í”¼ì²˜ë§µì„ í•™ìƒ ëª¨ë¸ì´ ì§ì ‘ ëª¨ë°©í•©ë‹ˆë‹¤.
3.  FaKD: êµì‚¬ ëª¨ë¸ í”¼ì²˜ë§µì˜ êµ¬ì¡°ì  ê´€ê³„(Affinity)ë¥¼ í•™ìƒ ëª¨ë¸ì´ ëª¨ë°©í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²• (ai/ ë””ë ‰í† ë¦¬ì—ì„œ):
    # FaKD ì‚¬ìš© ì˜ˆì‹œ
    python finetune_pruned_model.py --config config_catanet.yml \
                                     --teacher_weights weights/CATANet-L_x2.pth \
                                     --pruned_weights weights/catanet_pruned.pth \
                                     --save_path weights/catanet_finetuned_fakd.pth \
                                     --distillation_type fakd \
                                     --beta 100 
"""

def ensure_results_dirs():
    """ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    for subdir in ['loss', 'psnr', 'comparison']:
        os.makedirs(os.path.join(PLOTS_DIR, subdir), exist_ok=True)
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(SUMMARY_DIR, exist_ok=True)


def get_log_path(distillation_type):
    """ì¦ë¥˜ íƒ€ì…ì— ë”°ë¥¸ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    return os.path.join(LOGS_DIR, f"{distillation_type}_kd.csv")


def load_existing_log(log_path):
    """ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ë¡œë“œ (ì´ì–´ì„œ í•™ìŠµìš©)"""
    if os.path.exists(log_path):
        df = pd.read_csv(log_path)
        start_epoch = int(df['epoch'].max()) + 1
        print(f"ğŸ“‚ ê¸°ì¡´ ë¡œê·¸ ë°œê²¬! {start_epoch-1} ì—í­ë¶€í„° ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
        return df.to_dict('records'), start_epoch
    return [], 1


def save_log(log_data, log_path):
    """ë¡œê·¸ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
    df = pd.DataFrame(log_data)
    df.to_csv(log_path, index=False)


def generate_individual_plots(log_path, distillation_type):
    """ê°œë³„ ëª¨ë¸ì˜ Loss/PSNR ê·¸ë˜í”„ ìƒì„±"""
    df = pd.read_csv(log_path)

    # Loss ê·¸ë˜í”„
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(df['epoch'], df['total_loss'], label='Total Loss', color='blue')
    ax.plot(df['epoch'], df['task_loss'], label='Task Loss', color='green', alpha=0.7)
    ax.plot(df['epoch'], df['distill_loss'], label='Distill Loss', color='red', alpha=0.7)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title(f'{distillation_type.upper()} KD - Training Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'loss', f'{distillation_type}_kd_loss.png'), dpi=150)
    plt.close()

    # PSNR ê·¸ë˜í”„
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(df['epoch'], df['val_psnr'], label='Val PSNR', color='purple', linewidth=2)
    ax.axhline(y=df['val_psnr'].max(), color='red', linestyle='--', alpha=0.5, label=f'Best: {df["val_psnr"].max():.2f}')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('PSNR (dB)')
    ax.set_title(f'{distillation_type.upper()} KD - Validation PSNR')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'psnr', f'{distillation_type}_kd_psnr.png'), dpi=150)
    plt.close()

    print(f"ğŸ“Š {distillation_type} ê°œë³„ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ!")


def generate_comparison_plots():
    """3ê°€ì§€ KD ë°©ë²• ë¹„êµ ê·¸ë˜í”„ ìƒì„± (1000 ì—í­ ì™„ë£Œ ì‹œì—ë§Œ)"""
    kd_types = ['output', 'feature', 'fakd']
    colors = {'output': 'blue', 'feature': 'green', 'fakd': 'red'}

    # ëª¨ë“  ë¡œê·¸ íŒŒì¼ í™•ì¸
    all_logs = {}
    all_complete = True

    for kd_type in kd_types:
        log_path = get_log_path(kd_type)
        if os.path.exists(log_path):
            df = pd.read_csv(log_path)
            if len(df) >= 1000:
                all_logs[kd_type] = df
            else:
                print(f"â³ {kd_type} KD: {len(df)}/1000 ì—í­ (ë¯¸ì™„ë£Œ)")
                all_complete = False
        else:
            print(f"âš ï¸ {kd_type} KD ë¡œê·¸ íŒŒì¼ ì—†ìŒ")
            all_complete = False

    if not all_complete:
        print("âŒ 3ê°€ì§€ KD ë°©ë²• ëª¨ë‘ 1000 ì—í­ ì™„ë£Œ í›„ ë¹„êµ ê·¸ë˜í”„ê°€ ìƒì„±ë©ë‹ˆë‹¤.")
        return False

    print("âœ… ëª¨ë“  KD ë°©ë²• 1000 ì—í­ ì™„ë£Œ! ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")

    # PSNR ë¹„êµ ê·¸ë˜í”„
    fig, ax = plt.subplots(figsize=(12, 7))
    for kd_type, df in all_logs.items():
        ax.plot(df['epoch'], df['val_psnr'], label=f'{kd_type.upper()} KD',
                color=colors[kd_type], linewidth=1.5, alpha=0.8)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('PSNR (dB)', fontsize=12)
    ax.set_title('Knowledge Distillation Methods Comparison - PSNR', fontsize=14)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'comparison', 'all_psnr_comparison.png'), dpi=200)
    plt.close()

    # Loss ë¹„êµ ê·¸ë˜í”„
    fig, ax = plt.subplots(figsize=(12, 7))
    for kd_type, df in all_logs.items():
        ax.plot(df['epoch'], df['total_loss'], label=f'{kd_type.upper()} KD',
                color=colors[kd_type], linewidth=1.5, alpha=0.8)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Total Loss', fontsize=12)
    ax.set_title('Knowledge Distillation Methods Comparison - Loss', fontsize=14)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'comparison', 'all_loss_comparison.png'), dpi=200)
    plt.close()

    # ìµœì¢… ìš”ì•½ CSV ìƒì„±
    summary_data = []
    for kd_type, df in all_logs.items():
        summary_data.append({
            'method': f'{kd_type}_kd',
            'final_psnr': df['val_psnr'].iloc[-1],
            'best_psnr': df['val_psnr'].max(),
            'best_epoch': df.loc[df['val_psnr'].idxmax(), 'epoch'],
            'final_loss': df['total_loss'].iloc[-1],
            'min_loss': df['total_loss'].min()
        })

    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(os.path.join(SUMMARY_DIR, 'training_summary.csv'), index=False)

    print("ğŸ“Š ë¹„êµ ê·¸ë˜í”„ ë° ìš”ì•½ ì €ì¥ ì™„ë£Œ!")
    print(f"   - {os.path.join(PLOTS_DIR, 'comparison', 'all_psnr_comparison.png')}")
    print(f"   - {os.path.join(PLOTS_DIR, 'comparison', 'all_loss_comparison.png')}")
    print(f"   - {os.path.join(SUMMARY_DIR, 'training_summary.csv')}")

    return True


def calculate_fakd_loss(fm_teacher, fm_student):
    """
    Feature-Affinity based Knowledge Distillation (FaKD) ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    í”¼ì²˜ë§µì˜ 2ì°¨ í†µê³„ ì •ë³´(Gram í–‰ë ¬)ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    """
    # fm_teacher, fm_student shape: (B, C, H, W)
    
    # 1. í”¼ì²˜ë§µì„ (B, C, H*W) í˜•íƒœë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
    b, c, h, w = fm_teacher.shape
    fm_teacher_reshaped = fm_teacher.view(b, c, h * w)
    fm_student_reshaped = fm_student.view(b, c, h * w)

    # 2. ì±„ë„ ì°¨ì›ì„ ë”°ë¼ L2 ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    fm_teacher_normalized = F.normalize(fm_teacher_reshaped, p=2, dim=1)
    fm_student_normalized = F.normalize(fm_student_reshaped, p=2, dim=1)

    # 3. Gram í–‰ë ¬ (Affinity Matrix)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    # (B, C, N) -> (B, N, C)ë¡œ ì „ì¹˜ í›„ í–‰ë ¬ ê³±ì…ˆ
    affinity_teacher = torch.bmm(fm_teacher_normalized.transpose(1, 2), fm_teacher_normalized)
    affinity_student = torch.bmm(fm_student_normalized.transpose(1, 2), fm_student_normalized)
    
    # 4. ë‘ Affinity í–‰ë ¬ ê°„ì˜ L1 ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    loss = F.l1_loss(affinity_student, affinity_teacher)
    
    return loss

def main():
    # --- 0. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜ ë³´ì • ---
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    print(f"Working directory changed to: {os.getcwd()}")

    # --- ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„± ---
    ensure_results_dirs()

    # --- 1. ì¸ì íŒŒì‹± ë° ì„¤ì • ---
    parser = argparse.ArgumentParser(description="Pruned CATANet Fine-tuning with Knowledge Distillation")
    parser.add_argument("--config", required=True, help="ëª¨ë¸ ë° ë°ì´í„°ì…‹ ì„¤ì •ì„ ë‹´ì€ YAML íŒŒì¼")
    parser.add_argument("--teacher_weights", required=True, help="ì›ë³¸ êµì‚¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ")
    parser.add_argument("--pruned_weights", required=True, help="ê°€ì§€ì¹˜ê¸°ëœ í•™ìƒ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ")
    parser.add_argument("--save_path", required=True, help="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ")
    parser.add_argument("--epochs", type=int, default=10, help="íŒŒì¸íŠœë‹ ì—í­ ìˆ˜")
    parser.add_argument("--lr", type=float, default=1e-4, help="í•™ìŠµë¥ ")
    parser.add_argument("--alpha", type=float, default=0.8, help="Output Distillation Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--distillation_type", type=str, default="output", choices=["output", "feature", "fakd"], help="ì¦ë¥˜ íƒ€ì… ì„ íƒ")
    parser.add_argument("--beta", type=float, default=0.5, help="Feature/FaKD Distillation Loss ê°€ì¤‘ì¹˜")
    args = parser.parse_args()

    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    for key, value in config.items():
        if not hasattr(args, key):
            setattr(args, key, value)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # --- ë¡œê·¸ íŒŒì¼ ì„¤ì • (ì´ì–´ì„œ í•™ìŠµ ì§€ì›) ---
    log_path = get_log_path(args.distillation_type)
    log_data, start_epoch = load_existing_log(log_path)
    end_epoch = start_epoch + args.epochs - 1
    print(f"ğŸ“Š í•™ìŠµ ë²”ìœ„: {start_epoch} ~ {end_epoch} ì—í­")

    # --- 2. ë°ì´í„°ì…‹ ë¡œë“œ ---
    train_opt, val_opt = config['datasets']['train'], config['datasets']['val']
    train_opt['scale'], val_opt['scale'] = args.scale, args.scale
    train_opt['phase'], val_opt['phase'] = 'train', 'val'
    
    train_set = build_dataset(train_opt)
    val_set = build_dataset(val_opt)

    train_loader = DataLoader(train_set, batch_size=train_opt['batch_size_per_gpu'], shuffle=True, num_workers=train_opt.get('num_worker_per_gpu', 4), pin_memory=True)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=4)
    print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ. Train: {len(train_set)}ê°œ, Val: {len(val_set)}ê°œ")

    # --- 3. ëª¨ë¸ ë¡œë“œ ---
    print("êµì‚¬ ëª¨ë¸ ë¡œë”©...")
    teacher_model = get_catanet_teacher_model(weights_path=args.teacher_weights, upscale=args.scale).to(device)
    teacher_model.eval()

    print("í•™ìƒ ëª¨ë¸ ë¡œë”© (ê°€ì§€ì¹˜ê¸°ëœ ê°€ì¤‘ì¹˜)...")
    student_model = get_catanet_teacher_model(weights_path=None, upscale=args.scale).to(device)
    pruned_state = torch.load(args.pruned_weights, map_location=device)['params']
    student_model.load_state_dict(pruned_state, strict=False)
    student_model.train()
    
    teacher_hook = CATANetModelHooking(args=None, model=teacher_model)
    student_hook = CATANetModelHooking(args=None, model=student_model)

    # MODIFIED: 'feature' ë˜ëŠ” 'fakd'ì¼ ë•Œ hookì„ ë“±ë¡
    if args.distillation_type in ['feature', 'fakd']:
        print(f"'{args.distillation_type}' ì¦ë¥˜ë¥¼ ìœ„í•´ hookì„ í™œì„±í™”í•©ë‹ˆë‹¤.")
        teacher_hook.apply_mask_and_hooks()
        student_hook.apply_mask_and_hooks()
    
    print("ëª¨ë¸ ë¡œë“œ ë° í›„í‚¹ ì™„ë£Œ.")

    # --- 4. ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • ---
    optimizer = optim.AdamW(student_model.parameters(), lr=args.lr)
    l1_loss = torch.nn.L1Loss().to(device)

    # ê¸°ì¡´ ë¡œê·¸ì—ì„œ best_psnr ë³µì›
    best_psnr = max([d['val_psnr'] for d in log_data], default=0.0)
    if best_psnr > 0:
        print(f"ğŸ“ˆ ê¸°ì¡´ ìµœê³  PSNR: {best_psnr:.4f}")

    # --- 5. íŒŒì¸íŠœë‹ í•™ìŠµ ë£¨í”„ ---
    for epoch in range(start_epoch, end_epoch + 1):
        student_model.train()
        total_loss = 0
        total_task_loss = 0
        total_distill_loss = 0
        total_intermediate_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{end_epoch}", unit="batch")

        for batch in pbar:
            lq, gt = batch['lq'].to(device), batch['gt'].to(device)
            optimizer.zero_grad()

            with torch.no_grad():
                teacher_output, teacher_fms = teacher_hook.forwardPass(lq)
            student_output, student_fms = student_hook.forwardPass(lq)

            loss_task = l1_loss(student_output, gt)
            loss_distill_output = l1_loss(student_output, teacher_output)
            loss = (1 - args.alpha) * loss_task + args.alpha * loss_distill_output

            intermediate_loss = 0
            if args.distillation_type == 'feature' or args.distillation_type == 'fakd':
                if not (student_fms and teacher_fms):
                    print("ê²½ê³ : í”¼ì²˜ë§µì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì¤‘ê°„ ì¦ë¥˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                else:
                    for student_fm, teacher_fm in zip(student_fms, teacher_fms):
                        if args.distillation_type == 'feature':
                            intermediate_loss += l1_loss(student_fm, teacher_fm)
                        elif args.distillation_type == 'fakd':
                            intermediate_loss += calculate_fakd_loss(teacher_fm, student_fm)
                    loss += args.beta * intermediate_loss

            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            total_task_loss += loss_task.item()
            total_distill_loss += loss_distill_output.item()
            if isinstance(intermediate_loss, torch.Tensor):
                total_intermediate_loss += intermediate_loss.item()
            pbar.set_postfix({"Loss": f"{loss.item():.4f}"})

        avg_loss = total_loss / len(train_loader)
        avg_task_loss = total_task_loss / len(train_loader)
        avg_distill_loss = total_distill_loss / len(train_loader)
        avg_intermediate_loss = total_intermediate_loss / len(train_loader)
        print(f"Epoch {epoch} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f}")

        # --- 6. ê²€ì¦ (Validation) ---
        student_model.eval()
        current_psnr = 0
        with torch.no_grad():
            for batch in val_loader:
                lq, gt = batch['lq'].to(device), batch['gt'].to(device)
                student_output = student_model(lq)
                output_img, gt_img = tensor2img(student_output), tensor2img(gt)
                current_psnr += calculate_psnr(output_img, gt_img, crop_border=args.scale, test_y_channel=True)

        avg_psnr = current_psnr / len(val_loader)
        print(f"ê²€ì¦ ì™„ë£Œ. í‰ê·  PSNR: {avg_psnr:.4f}")

        # --- ë¡œê·¸ ê¸°ë¡ ---
        log_data.append({
            'epoch': epoch,
            'total_loss': avg_loss,
            'task_loss': avg_task_loss,
            'distill_loss': avg_distill_loss,
            'intermediate_loss': avg_intermediate_loss,
            'val_psnr': avg_psnr,
            'lr': args.lr,
            'alpha': args.alpha,
            'beta': args.beta
        })
        save_log(log_data, log_path)

        if avg_psnr > best_psnr:
            best_psnr = avg_psnr
            os.makedirs(os.path.dirname(args.save_path), exist_ok=True)
            torch.save(student_model.state_dict(), args.save_path)
            print(f"ìµœê³  ì„±ëŠ¥ ë‹¬ì„±! ëª¨ë¸ì„ '{args.save_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (PSNR: {best_psnr:.4f})")

        # í›… ì¬ì„¤ì • (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        if args.distillation_type in ['feature', 'fakd']:
            teacher_hook.purge_hooks()
            student_hook.purge_hooks()
            if epoch < end_epoch:
                 teacher_hook.apply_mask_and_hooks()
                 student_hook.apply_mask_and_hooks()

    print(f"\n--- íŒŒì¸íŠœë‹ ì™„ë£Œ ---\nìµœê³  PSNR: {best_psnr:.4f}")
    print(f"ìµœì¢… ëª¨ë¸ì€ '{args.save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š ë¡œê·¸ ì €ì¥ë¨: {log_path}")

    # --- 7. ê·¸ë˜í”„ ìƒì„± ---
    current_total_epochs = len(log_data)
    print(f"\nğŸ“ˆ í˜„ì¬ ì´ ì—í­: {current_total_epochs}/1000")

    # ê°œë³„ ê·¸ë˜í”„ëŠ” í•­ìƒ ìƒì„±
    generate_individual_plots(log_path, args.distillation_type)

    # 1000 ì—í­ ë‹¬ì„± ì‹œ ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì‹œë„
    if current_total_epochs >= 1000:
        print("ğŸ‰ 1000 ì—í­ ë‹¬ì„±! ë¹„êµ ê·¸ë˜í”„ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        generate_comparison_plots()


if __name__ == '__main__':
    main()